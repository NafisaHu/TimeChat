{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install omegaconf\n",
    "%pip install iopath\n",
    "%pip install timm\n",
    "%pip install decord\n",
    "%pip install webdataset\n",
    "%pip install einops\n",
    "%pip install wandb\n",
    "%pip install gradio\n",
    "%pip install torchshow\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchshow as ts\n",
    "from timechat.common.config import Config\n",
    "from timechat.common.dist_utils import get_rank\n",
    "from timechat.common.registry import registry\n",
    "from timechat.conversation.conversation_video import Chat, Conversation, default_conversation,SeparatorStyle, conv_llava_llama_2\n",
    "import decord\n",
    "import cv2\n",
    "import time\n",
    "import subprocess\n",
    "from decord import VideoReader\n",
    "from timechat.processors.video_processor import ToTHWC, ToUint8, load_video\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "# imports modules for registration\n",
    "from timechat.datasets.builders import *\n",
    "from timechat.models import *\n",
    "from timechat.processors import *\n",
    "from timechat.runners import *\n",
    "from timechat.tasks import *\n",
    "\n",
    "import random as rnd\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from PIL import Image\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--cfg-path\", default='eval_configs/timechat.yaml', help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--num-beams\", type=int, default=1)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--text-query\", default=\"What is he doing?\", help=\"question the video\")\n",
    "    parser.add_argument(\"--video-path\", default='examples/vein.mp4', help=\"path to video file.\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing Chat')\n",
    "args = parse_args()\n",
    "cfg = Config(args)\n",
    "\n",
    "DIR=\"ckpt/timechat\"\n",
    "MODEL_DIR=f\"{DIR}/timechat_7b.pth\"\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_config.ckpt = MODEL_DIR\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "model.eval()\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.webvid.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n",
    "print('Initialization Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ffmpeg\n",
    "import os\n",
    "\n",
    "def extract_segment(input_file, output_file, start_time, end_time):\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(input_file, ss=start_time)\n",
    "        .output(output_file, to=end_time-start_time)\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "def parse_timestamps(llm_message):\n",
    "    segments = []\n",
    "    for match in re.finditer(r\"(\\d+\\.\\d+) - (\\d+\\.\\d+) seconds, (.+?)(?=(?:\\s*\\d+\\.\\d+ -)|$)\", llm_message):\n",
    "        start_time = float(match.group(1))\n",
    "        end_time = float(match.group(2))\n",
    "        description = match.group(3).strip()\n",
    "        segments.append((start_time, end_time, description))\n",
    "    return segments\n",
    "\n",
    "def extract_segments(input_file, llm_message):\n",
    "    segments = parse_timestamps(llm_message)\n",
    "    \n",
    "    dvc_prompt_folder = \"segmented_videos\"\n",
    "    highlight_prompt_folder = \"segmented_videos_highlight\"\n",
    "    slc_prompt_folder = \"segmented_videos_slc_2\"\n",
    "    summarization_prompt_folder = \"segmented_videos_summ\"\n",
    "\n",
    "    output_folder = os.path.join(summarization_prompt_folder, os.path.splitext(os.path.basename(input_file))[0])\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    summary_file = os.path.join(output_folder, \"segmentsummary.txt\")\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        for idx, (start_time, end_time, description) in enumerate(segments):\n",
    "            output_file = os.path.join(output_folder, f\"output_{idx}.mp4\")\n",
    "            extract_segment(input_file, output_file, start_time, end_time)\n",
    "            print(f\"Segment {idx+1}: {description} (from {start_time} to {end_time}) extracted to {output_file}\")\n",
    "            f.write(f\"Segment {idx+1}: {description} (from {start_time} to {end_time}) extracted to {output_file}\\n\")\n",
    "\n",
    "input_folder = 'examples'\n",
    "youcook_prompt_dvc = \"You are given a cooking video from the YouCook2 dataset. Please watch the video and extract a maximum of 10 significant cooking steps. For each step, determine the starting and ending times and provide a concise description. The format should be: 'start time - end time, brief step description'. For example, ' 90 - 102 seconds, spread margarine on two slices of white bread'.\"\n",
    "highlight_prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. The output format of each predicted event should be like: 'start - end seconds, event description'. A specific example is : ' 90 - 102 seconds, spread margarine on two slices of white bread in the video'.\"\n",
    "slc_prompt = \"Identify and mark the video segments corresponding to a series of actions or steps, specifying the timestamps and describing the steps.\"\n",
    "summarization_prompt = \"Generate a summarized version of the video, focusing on extracting key frames that best represent the overall narrative. The output should be a list of timestamps in seconds and their corresponding salient scores.\"\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        video, _ = load_video(\n",
    "            video_path=video_path,\n",
    "            n_frms=32,\n",
    "            sampling=\"uniform\",\n",
    "            return_msg=True\n",
    "        )\n",
    "        # video = vis_processor.transform(video)\n",
    "        print(video.size())\n",
    "        C, T, H, W = video.shape\n",
    "        ts.show(video.transpose(0, 1))\n",
    "\n",
    "        img_list = []\n",
    "        chat_state = conv_llava_llama_2.copy()\n",
    "        chat_state.system = \"You are able to understand the visual content that the user provides. Follow the instructions carefully and explain your answers in detail.\"\n",
    "        msg = chat.upload_video_without_audio(\n",
    "            video_path=video_path,\n",
    "            conv=chat_state,\n",
    "            img_list=img_list,\n",
    "            n_frms=96,\n",
    "        )\n",
    "\n",
    "        text_input = summarization_prompt\n",
    "        print(text_input)\n",
    "\n",
    "        chat.ask(text_input, chat_state)\n",
    "\n",
    "        num_beams = args.num_beams\n",
    "        temperature = args.temperature\n",
    "        llm_message = chat.answer(conv=chat_state,\n",
    "                                  img_list=img_list,\n",
    "                                  num_beams=num_beams,\n",
    "                                  temperature=temperature,\n",
    "                                  max_new_tokens=300,\n",
    "                                  max_length=2000)[0]\n",
    "\n",
    "        print(llm_message)\n",
    "\n",
    "        extract_segments(video_path, llm_message)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
